{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb091df-44bd-485f-8b55-aeb1a58b403b",
   "metadata": {},
   "source": [
    "### Deep LEarning Curriculum\n",
    "\n",
    "#### Module 8: Deep Learning Fundamentals\n",
    "##### LO 8.1: Understand Neural Networks\n",
    "- PC 8.1.1: Define artificial neural networks and their components (neurons, layers). \n",
    "- PC 8.1.2: Understand activation functions (ReLU, Sigmoid, Softmax).\n",
    "- PC 8.1.3: Implement feedforward neural networks using Keras.\n",
    "\n",
    "##### LO 8.2: Train Deep Learning Models\n",
    "- PC 8.2.1: Use backpropagation and gradient descent for training models.\n",
    "- PC 8.2.2: Understand overfitting and apply regularization techniques.\n",
    "- PC 8.2.3: Implement convolutional neural networks (CNN) for image recognition tasks.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdd1a9-5c2d-452c-965b-2411d168e980",
   "metadata": {},
   "source": [
    "#### What is deep learning?\n",
    "- Deep Learning (DL) is a subfield of Machine Learning\n",
    "- It is structured using the human neural system\n",
    "- It consists of layers\n",
    "\n",
    "#### Why we do Deep Learning?\n",
    "1. Machine Learning models do not perform very well with unstructured data\n",
    "2. Machine Learning models stop giving accurate predictions when data reached a certain threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87594f-0645-4eae-b730-b339d80afafa",
   "metadata": {},
   "source": [
    "### What is Artificial Neural Network (ANN):\n",
    "\n",
    "An artificial neural network (ANN) is a machine learning algorithm that uses a network of interconnected nodes to process data like the human brain. ANNs are a type of deep learning that can help computers learn and make decisions in a human-like way. \n",
    "\n",
    "##### How it works \n",
    "- ANNs are made up of layers of nodes, or neurons, that process and transmit information.\n",
    "- ANNs use complex algorithms to determine the strength of each neuron and its relationship to other neurons.\n",
    "- ANNs use predicted and actual outputs to improve their function. This process is called \"training\".\n",
    "\n",
    "##### What it can do \n",
    "- ANNs can recognize complex patterns.\n",
    "- ANNs can learn from changing sets of data.\n",
    "- ANNs can make predictions in real time.\n",
    "- ANNs can solve complicated problems, like summarizing documents or recognizing faces.\n",
    "\n",
    "Examples of ANN applications Facial recognition, Real-time translation, Google photos, Autonomous cars, and Generative AI. \n",
    "\n",
    "### What is an activationn Function\n",
    "An activation function in deep learning is a mathematical function that determines whether a neuron should be activated. It transforms the input signal of a neuron into an output signal that is passed on to the next layer. \n",
    "\n",
    "##### Why are activation functions important? \n",
    "- They allow neural networks to learn complex patterns in data\n",
    "- They enable neural networks to model non-linear relationships between inputs and outputs\n",
    "- They are crucial for training neural networks that generalize well and provide accurate predictions\n",
    "\n",
    "##### How do activation functions work?\n",
    "- A node receives a set of input signals \n",
    "- The activation function decides whether the neuron should be activated or not \n",
    "- The activation function transforms the input signal into an output signal \n",
    "- The output signal is passed on to the next layer \n",
    "\n",
    "##### What are some examples of activation functions?\n",
    "- Sigmoid: Maps inputs to a range between zero and one \n",
    "- Tanh: A shifted version of the sigmoid that outputs values from -1 to +1 \n",
    "- Swish: A smooth activation function that bends from 0 towards values < 0 and then upwards again "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e409e522-e4ae-4524-abaf-41d37e8acefe",
   "metadata": {},
   "source": [
    "The two most popularly used activatio functions are Sigmoid and ReLU\n",
    "\n",
    "### Sigmoid Activation Function\n",
    "The sigmoid activation function is a mathematical function that converts a real number input into a value between 0 and 1. It's used in neural networks to control the output of a layer. \n",
    "\n",
    "##### How it works\n",
    "- Inputs: Small inputs result in outputs close to 0, while large inputs result in outputs close to 1. \n",
    "- Shape: The sigmoid function has an \"S\"-shaped curve. \n",
    "- Outputs: The outputs can be interpreted as probabilities, making it useful for classification and probability prediction. \n",
    "\n",
    "##### Why it's used \n",
    "- The sigmoid function was historically important in the development of neural networks.\n",
    "- It's useful for predicting probabilities, which makes it natural for binary classification problems.\n",
    "\n",
    "##### Limitations\n",
    "- The sigmoid function has some inefficiencies that have reduced its usage in more recent years. \n",
    "- The sigmoid function can suffer from the \"vanishing gradient\" problem, which can make learning in deep neural networks slow or even halt it. \n",
    "- All the variants of the sigmoid function are computationally intensive to calculate. \n",
    "\n",
    "##### Alternatives \n",
    "The ReLU function is a faster alternative to the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac405b8e-4533-4457-ad7a-76d652ee374f",
   "metadata": {},
   "source": [
    "### ReLU Activation Function\n",
    "The rectified linear unit (ReLU) activation function is a mathematical function used in artificial neural networks to introduce non-linearity. It's also known as the rectifier activation function. \n",
    "\n",
    "##### How it works\n",
    "- If the input value is positive, the ReLU function outputs the same value. \n",
    "- If the input value is negative, the ReLU function outputs zero. \n",
    "\n",
    "##### Benefits\n",
    "- Non-linearity: ReLU introduces non-linearity to the network, which allows it to learn complex patterns in the data. \n",
    "- Computational efficiency: ReLU is computationally inexpensive because it only activates a subset of neurons at a time. \n",
    "- Generalization: ReLU allows neural networks to generalize better to unseen data. \n",
    "- Mitigates vanishing gradient problem: ReLU's linear property for positive inputs helps mitigate the vanishing gradient problem. \n",
    "\n",
    "##### Applications \n",
    "computer vision, speech recognition, and computational neuroscience. \n",
    "\n",
    "##### Drawbacks \n",
    "Dying ReLU: ReLU can become \"dead\" if it's trapped on the negative side and always outputs zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af0dce1-f14c-4a49-a3e9-a444c15b4cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 15:03:28.191045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ashpot/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#import required packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#generate data\n",
    "#generate train dummy data for 1000 students and dummy test for 500\n",
    "\n",
    "#columns: age, hours of study and average previous test scores\n",
    "\n",
    "#set seed\n",
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e3d944-b611-4b26-9f65-866a9cbe7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = np.random.random((1000,3)), np.random.random((500,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a00f5d7-a770-4255-8902-a3bb1396e5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94763226, 0.22654742, 0.59442014],\n",
       "       [0.42830868, 0.76414069, 0.00286059],\n",
       "       [0.35742368, 0.90969489, 0.45608099],\n",
       "       ...,\n",
       "       [0.13403753, 0.56654667, 0.06479317],\n",
       "       [0.62502594, 0.02876147, 0.70924028],\n",
       "       [0.87752115, 0.20447666, 0.32131087]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a848220-7a86-4139-91c7-25a77151cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate dummy results for 1000 students: Passed (1) or Failed (0)\n",
    "labels = np.random.randint(2,size=(1000,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82730bc4-59f5-4acb-9f5c-9b9debf87c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashpot/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define the model structure with  required layers of neurons\n",
    "activation functions and optimizers\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=3, activation='relu'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49dc5fc-508d-4a72-b77c-058594f9f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4793 - loss: 0.7010 \n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4806 - loss: 0.6994 \n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4547 - loss: 0.6977 \n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4857 - loss: 0.6954 \n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4442 - loss: 0.6964\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4887 - loss: 0.6952\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4999 - loss: 0.6934\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5068 - loss: 0.6927\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4882 - loss: 0.6930\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4684 - loss: 0.6944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x136b84880>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model and male predictions\n",
    "model.fit(train_data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280d11bf-0e39-4b8d-97f2-d95a20dbf682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "predictions = model.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
